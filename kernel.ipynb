{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of kernel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "DSc8sd1R-A4u",
        "SJR7Qakp-A59",
        "gv4ZroxB-A9a",
        "H0HS6oFp-A9m",
        "Kzv7TnOm-A-L",
        "OUH5ER5k-A-P",
        "VRDH6JoH-A-Z",
        "7C_TgddY-A_V",
        "iJZMJqIV-A_l",
        "PzsCgDge-A_x",
        "ahNXYTOf-A_5",
        "WYozSKVZ-BAF",
        "_g2XpIYw-BAU",
        "HUBmD2Gi-BAf",
        "F1poPKde-BBC",
        "sJ7Li0B5-BBZ",
        "IK4NHCVi-BBx",
        "XSmkUKIA-BB4",
        "gaX5qpEz-BB9",
        "aHfk6PXE-BCI",
        "uDfQPHfZ-BCs",
        "QyQ1vj7d-BDI",
        "TS6xtGNp-BDQ",
        "gRVfGLui-BDj",
        "fyjS8gln-BGa",
        "7dMEpb-V-BG0",
        "tsj0vXaS-BHV",
        "b8EENeyd-BHg",
        "unPcz0A1-BHq",
        "YM68Gia7-BHs",
        "PD8lqE-M-BHw",
        "BfT8PT8J-BH4",
        "PK0WNrQy-BH_",
        "Q1i7rfd6-BIH",
        "jjFkN-UX-BIN",
        "g4NYZrk7-BIS",
        "-IY6qjb0-BJz",
        "gPrGWNhN-BKJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "_uuid": "a8f9622945156d6337ba73c481da2de7efef7384",
        "id": "ZWqX471j-A3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# <div align=\"center\">  Artificial Intelligence Concordia Workshop 1: <br /> </div>\n",
        "## <div align=\"center\"> Basics of Machine Learning, Linear Regression and Logistic Regression</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/olibel270/Workshop1JupyterNote/blob/master/images/AICLogo.jpg?raw=1\" style=\"width: 300px\" /></div>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n",
        "<div align=\"right\">\n",
        "  Follow us on:<br />\n",
        "[ Facebook](https://www.facebook.com/AISConU/)<br />\n",
        "[Our Website](https://www.aisconcordia.com)<br />\n",
        "  </div>\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "FPuEX-v_O4g7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ]
    },
    {
      "metadata": {
        "id": "s5KjdaB7Dio9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Machine Learning as a subset of AI"
      ]
    },
    {
      "metadata": {
        "id": "41ecLXDFz6hN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Python"
      ]
    },
    {
      "metadata": {
        "id": "yQaehA03AOrW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Short description <br />\n",
        "  Tools Available:\n",
        "  Many libraries: Scikit-Learn Numpy, Pytorch\n",
        "  Highly optimized implementations to make use of multi-cores and GPUs\n",
        "  Community of users / Ease of finding information\n",
        "  \n",
        "  https://realpython.com/python-first-steps/\n",
        "  \n",
        "  The Google Colab Environment"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "aY1DxfDg-A4U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import Python Packages\n",
        "import warnings\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Relevant Libraries for ML\n",
        "import csv\n",
        "import scipy\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "\n",
        "# Show Packages versions\n",
        "print('Installed packages and their versions:')\n",
        "print('csv: {}'.format(csv.__version__))\n",
        "print('scipy: {}'.format(scipy.__version__))\n",
        "print('matplotlib: {}'.format(matplotlib.__version__))\n",
        "print('numpy: {}'.format(np.__version__))\n",
        "print('pandas: {}'.format(pd.__version__))\n",
        "print('seaborn: {}'.format(sns.__version__))\n",
        "print('matplotlib: {}'.format(matplotlib.__version__))\n",
        "print('sklearn: {}'.format(sklearn.__version__))\n",
        "\n",
        "# Setting some Parameters\n",
        "sns.set(color_codes=True)\n",
        "%matplotlib inline\n",
        "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BMt7IWKMDxc-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing metrics for evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew #for some statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r22tdM7-zs8_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# General Framework of Machine Learning"
      ]
    },
    {
      "metadata": {
        "id": "RUaSqWNwD3wB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"8\"></a> <br>\n",
        "### 4-1 Inputs\n",
        "* train.csv - the training set\n",
        "\n",
        "<a id=\"9\"></a> <br>\n",
        "\n",
        "### 4-2 Outputs\n",
        "* sale prices for every record in test.csv\n",
        "\n",
        "* Define Problem \n",
        "* Specify Inputs & Outputs\n",
        "* Exploratory data analysis\n",
        "* Data Collection\n",
        "* Data Preprocessing\n",
        "* Data Cleaning\n",
        "* Visualization\n",
        "* Model Design, Training, and Offline Evaluation\n",
        "* Model Deployment, Online Evaluation, and Monitoring\n",
        "* Model Maintenance, Diagnosis, and Retraining\n",
        "\n",
        " <img src=\"http://s9.picofile.com/file/8338227634/workflow.png\" />\n",
        " \n",
        "Regression Vs. Classification\n",
        "\n",
        "\n",
        "## 2-1 How to solve Problem?\n",
        "**Data Science has so many techniques and procedures that can confuse anyone.**\n",
        "\n",
        "**Step 1**: Translate your business problem statement into technical one\n",
        "\n",
        "**Step 2**: Decide on the supervised learning technique\n",
        "\n",
        "Classification Vs. regression\n",
        "\n",
        "**Step 3**: Literature survey\n",
        "\n",
        "**Step 4**: Data cleaning\n",
        "\n",
        "Missing values\n",
        "\n",
        "different techniques to impute missing values \n",
        "\n",
        "Duplicate records\n",
        "\n",
        "Incorrect values\n",
        "3 standard deviations from the mean\n",
        "\n",
        "**Step 5**: Feature engineering\n",
        "\n",
        "Removing redundant features\n",
        " metrics like AIC and BIC to identify redundant features. There are built in packages to perform operations like forward selection, backward selection etc. to remove redundant features.\n",
        "\n",
        "Transforming a feature\n",
        "A feature might have a non linear relationship with the output column. While complex models can capture this with enough data, simple models might not be able to capture this. I usually try to visualize different functions of each column like log, inverse, quadratic, cubic etc. and choose the transformation that looks closest to a normal curve.\n",
        "\n",
        "**Step 6**: Data modification\n",
        "\n",
        "Scaling\n",
        "Skew\n",
        "Up-sample\n",
        "Down-sample\n",
        "\n",
        "**Step 7**: Modelling\n",
        "\n",
        "Start with simple models, then more complex ones\n",
        "\n",
        "Knowledge of the assumptions of each models\n",
        "\n",
        "**Step 8**: Model comparison\n",
        "Cross validation basically brings out an average performance of a model. avoid over-fitting. \n",
        "randomize data before cross validation.\n",
        "\n",
        "A good technique to compare performance of different models is ROC curves. ROC curves help you visualize performance of different models across different thresholds. While ROC curves give a holistic sense of model performance, based on the business decision, you must choose the performance metric like Accuracy, True Positive Rate, False Positive Rate, F1-Score etc.\n",
        "\n",
        "**Step 9**: Error analysis\n",
        "\n",
        "\n",
        "**Step 10**: Improving your best model\n",
        "\n",
        "Once I have the best model, I usually plot training vs testing accuracy [or the right metric] against the number of parameters. Usually, it is easy to check training and testing accuracy against number of data points. Basically this plot will tell you whether your model is over-fitting or under-fitting. This articleDetecting over-fitting vs under-fitting explains this concept clearly.\n",
        "\n",
        "Understanding if your model is over-fitting or under-fitting will tell you how to proceed with the next steps. If the model is over-fitting, you might consider collecting more data. If the model is under-fitting, you might consider making the models more complex. [Eg. Adding higher order terms to a linear / logistic regression]\n",
        "\n",
        "**Step 11**: Deploying the model\n",
        "\n",
        "\n",
        "**Step 12**: Adding feedback\n",
        " historical data. \n",
        "capture the current trends or changes\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xn9DGzlP2JfA"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem Definition\n"
      ]
    },
    {
      "metadata": {
        "id": "kz77Rv8_3OA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we will use the house prices data set. This dataset contains information about house prices and the target value is:\n",
        "\n",
        "* SalePrice"
      ]
    },
    {
      "metadata": {
        "id": "XDvE_cWLDxIo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "vtPRm-JKAbks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "  ## Datasets"
      ]
    },
    {
      "metadata": {
        "id": "-Q485TIOETY4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/\n",
        "\n",
        "https://www.kaggle.com/datasets\n"
      ]
    },
    {
      "metadata": {
        "id": "aOBmW0p8-2u7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Housing Dataset\n",
        "  \n",
        "\n",
        "<img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/5407/media/housesbanner.png\"></img>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DYzTG1Qm_pUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Variables\n"
      ]
    },
    {
      "metadata": {
        "id": "5_xRNfnFBXz2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The variables are :\n",
        "* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
        "* MSSubClass: The building class\n",
        "* MSZoning: The general zoning classification\n",
        "* LotFrontage: Linear feet of street connected to property\n",
        "* LotArea: Lot size in square feet\n",
        "* Street: Type of road access\n",
        "* Alley: Type of alley access\n",
        "* LotShape: General shape of property\n",
        "* LandContour: Flatness of the property\n",
        "* Utilities: Type of utilities available\n",
        "* LotConfig: Lot configuration\n",
        "* LandSlope: Slope of property\n",
        "* Neighborhood: Physical locations within Ames city limits\n",
        "* Condition1: Proximity to main road or railroad\n",
        "* Condition2: Proximity to main road or railroad (if a second is present)\n",
        "* BldgType: Type of dwelling\n",
        "* HouseStyle: Style of dwelling\n",
        "* OverallQual: Overall material and finish quality\n",
        "* OverallCond: Overall condition rating\n",
        "* YearBuilt: Original construction date\n",
        "* YearRemodAdd: Remodel date\n",
        "* RoofStyle: Type of roof\n",
        "* RoofMatl: Roof material\n",
        "* Exterior1st: Exterior covering on house\n",
        "* Exterior2nd: Exterior covering on house (if more than one material)\n",
        "* MasVnrType: Masonry veneer type\n",
        "* MasVnrArea: Masonry veneer area in square feet\n",
        "* ExterQual: Exterior material quality\n",
        "* ExterCond: Present condition of the material on the exterior\n",
        "* Foundation: Type of foundation\n",
        "* BsmtQual: Height of the basement\n",
        "* BsmtCond: General condition of the basement\n",
        "* BsmtExposure: Walkout or garden level basement walls\n",
        "* BsmtFinType1: Quality of basement finished area\n",
        "* BsmtFinSF1: Type 1 finished square feet\n",
        "* BsmtFinType2: Quality of second finished area (if present)\n",
        "* BsmtFinSF2: Type 2 finished square feet\n",
        "* BsmtUnfSF: Unfinished square feet of basement area\n",
        "* TotalBsmtSF: Total square feet of basement area\n",
        "* Heating: Type of heating\n",
        "* HeatingQC: Heating quality and condition\n",
        "* CentralAir: Central air conditioning\n",
        "* Electrical: Electrical system\n",
        "* 1stFlrSF: First Floor square feet\n",
        "* 2ndFlrSF: Second floor square feet\n",
        "*  LowQualFinSF: Low quality finished square feet (all floors)\n",
        "* GrLivArea: Above grade (ground) living area square feet\n",
        "* BsmtFullBath: Basement full bathrooms\n",
        "* BsmtHalfBath: Basement half bathrooms\n",
        "* FullBath: Full bathrooms above grade\n",
        "* HalfBath: Half baths above grade\n",
        "* Bedroom: Number of bedrooms above basement level\n",
        "* Kitchen: Number of kitchens\n",
        "* KitchenQual: Kitchen quality\n",
        "* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
        "* Functional: Home functionality rating\n",
        "* Fireplaces: Number of fireplaces\n",
        "* FireplaceQu: Fireplace quality\n",
        "* GarageType: Garage location\n",
        "* GarageYrBlt: Year garage was built\n",
        "* GarageFinish: Interior finish of the garage\n",
        "* GarageCars: Size of garage in car capacity\n",
        "* GarageArea: Size of garage in square feet\n",
        "* GarageQual: Garage quality\n",
        "* GarageCond: Garage condition\n",
        "* PavedDrive: Paved driveway\n",
        "* WoodDeckSF: Wood deck area in square feet\n",
        "* OpenPorchSF: Open porch area in square feet\n",
        "* EnclosedPorch: Enclosed porch area in square feet\n",
        "* 3SsnPorch: Three season porch area in square feet\n",
        "* ScreenPorch: Screen porch area in square feet\n",
        "* PoolArea: Pool area in square feet\n",
        "* PoolQC: Pool quality\n",
        "* Fence: Fence quality\n",
        "* MiscFeature: Miscellaneous feature not covered in other categories\n",
        "* MiscVal: $Value of miscellaneous feature\n",
        "* MoSold: Month Sold\n",
        "* YrSold: Year Sold\n",
        "* SaleType: Type of sale\n",
        "* SaleCondition: Condition of sale"
      ]
    },
    {
      "metadata": {
        "id": "e-sPjwjj04VS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "metadata": {
        "id": "YM8xov3Km_BC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists('Workshop1JupyterNote'):\n",
        "  ! git clone https://github.com/olibel270/Workshop1JupyterNote.git --quiet\n",
        "\n",
        "%cd Workshop1JupyterNote\n",
        "!git checkout master --quiet\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "print(\"Checking whether the dataset import properly:\")\n",
        "print(check_output([\"ls\", \"./dataset\"]).decode(\"utf8\")) #check the files available in the directory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIbVTmTVFYLD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Challenges"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ANRrGYLu2Je8"
      },
      "cell_type": "markdown",
      "source": [
        "1- Attributes are numeric and categorical so you have to figure out how to load and handle data.\n",
        "\n",
        "2- It is a Regression problem, allowing you to practice with perhaps an easier type of supervised learning algorithm."
      ]
    },
    {
      "metadata": {
        "id": "ySk0sGM9AxLi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Representing Data"
      ]
    },
    {
      "metadata": {
        "id": "MOvmikuVJ4Ta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Numpy"
      ]
    },
    {
      "metadata": {
        "id": "vM2ab8sVJc7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_set = open('dataset/train.csv')\n",
        "csv_training_set = csv.DictReader(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bnVkoq61J3yp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pandas"
      ]
    },
    {
      "metadata": {
        "_uuid": "9269ae851b744856bce56840637030a16a5877e1",
        "id": "Pt0a4V4j-A4y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('dataset/train.csv')\n",
        "test= pd.read_csv('dataset/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zIT9F0mGZJ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exploring the Dataset"
      ]
    },
    {
      "metadata": {
        "_uuid": "581b90e6a869c3793472c7edd59091d6d6342fb2",
        "id": "SJR7Qakp-A59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using Pandas and the documentation can you find out:\n",
        "\n",
        "1- Dimensions of the dataset.\n",
        "\n",
        "2- Peek at the data itself.\n",
        "\n",
        "3- Statistical summary of all attributes.[link text](https://)\n",
        "\n",
        "4- Breakdown of the data by the class variable.[7]\n",
        "\n",
        "Pandas Cheat Sheet:\n",
        "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf"
      ]
    },
    {
      "metadata": {
        "_uuid": "4b45251be7be77333051fe738639104ae1005fa5",
        "colab_type": "code",
        "id": "DVPBCXUvMEVO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shape\n",
        "print(train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "145f02fa22f216885a59cddee46dc08550426fa9",
        "colab_type": "code",
        "id": "JNbzSGytMEVB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shape\n",
        "print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca840f02925751186f87e402fcb5f637ab1ab8a0",
        "id": "2mipdi96-A6r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5899889553c3416b27e93efceddb106eb71f5156",
        "id": "1C7QIYIf-A7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.head(5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "79339442ff1f53ae1054d794337b9541295d3305",
        "id": "6t1ASkGD-A7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.tail() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "09eb18d1fcf4a2b73ba2f5ddce99dfa521681140",
        "id": "Qt5ca7fJ-A71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.sample(5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f7211e96627b9a81c5b620a9ba61446f7719ea3",
        "id": "luvqSBaj-A8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.describe() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8807b632269e2fa734ad26e8513199400fc09a83",
        "id": "CIfeH-KT-A8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "446e6162e16325213047ff31454813455668b574",
        "id": "rEWOaH4x-A8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.groupby('SaleType').count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "909d61b33ec06249d0842e6115597bbacf21163f",
        "id": "wwNE3Ry7-A88",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b545ff7e8367c5ab9c1db710f70b6936ac8422c",
        "id": "Nc1HXydv-A9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train[train['SalePrice']>700000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7dE3HjSzAsFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing Data\n"
      ]
    },
    {
      "metadata": {
        "id": "rGYFlFN1F_mr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Scatter plot"
      ]
    },
    {
      "metadata": {
        "_uuid": "b0014a7a52e714996bc443981c853095926d20e5",
        "id": "OUH5ER5k-A-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Scatter plot Purpose To identify the type of relationship (if any) between two quantitative variables\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "af099546eed64ebc796403d4139cb4c977c27b03",
        "id": "Kdtqj1u1-A-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = ['SalePrice','OverallQual']\n",
        "sns.FacetGrid(train[columns], size=5).map(plt.scatter, \"OverallQual\", \"SalePrice\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "af099546eed64ebc796403d4139cb4c977c27b03",
        "colab_type": "code",
        "id": "IE-QIzcRDdWK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = ['SalePrice','1stFlrSF']\n",
        "sns.FacetGrid(train[columns], size=5).map(plt.scatter, \"1stFlrSF\", \"SalePrice\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHNVnwYVGEPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multivariate Plots"
      ]
    },
    {
      "metadata": {
        "_uuid": "3bbff56707484f88625eb8ef309b712ba03f939e",
        "id": "iJZMJqIV-A_l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can look at the interactions between the variables.\n",
        "\n",
        "First, let’s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables."
      ]
    },
    {
      "metadata": {
        "_uuid": "eb4e5d117e4ef40d7668632f42130206a5537bd0",
        "id": "MLItpdBF-A_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\n",
        "pd.plotting.scatter_matrix(train[columns],figsize=(12,12))\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e73333289d17dd648b7b2112d7fe3fe7ea444d0",
        "id": "F1poPKde-BBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Heatmap"
      ]
    },
    {
      "metadata": {
        "_uuid": "e798f483a3a92e37830225d644a9656e34dcd5d6",
        "id": "j098VPV--BGv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Adding total sqfootage feature \n",
        "train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3100955ca9dc61ac7d435e9c064d10d06f26afa7",
        "id": "7JnH5_Ax-BBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,4)) \n",
        "columns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd', 'TotalSF']\n",
        "sns.heatmap(train[columns].corr(),annot=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EEhbIny9GNN1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Histogram"
      ]
    },
    {
      "metadata": {
        "_uuid": "743a92c3c2fff1a1f99845518247f7971ad18b7c",
        "id": "7C_TgddY-A_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also create a **histogram** of each input variable to get an idea of the distribution.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "5da0520ed3e738ee8814b2d91843ed4acec2b6e6",
        "id": "yxhFZjKW-A_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# histograms\n",
        "train.hist(figsize=(15,20))\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fb187bcc0fb51e53f8abe9e3952c6ae5c3177411",
        "id": "jcem6iK3-BAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the plot, we can see that the species setosa is separataed from the other two across all feature combinations\n",
        "\n",
        "We can also replace the histograms shown in the diagonal of the pairplot by kde."
      ]
    },
    {
      "metadata": {
        "id": "VBzkK2ToAiEi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Scraping Data (TO CLEAN AND COMPLETE)"
      ]
    },
    {
      "metadata": {
        "id": "BQlB9a7QAmGW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Literature survey\n",
        "\n",
        "  \n",
        "  Step 4: Data cleaning\n",
        "\n",
        "If you speak with anyone who has spent some time in data science, they will always say that most of their time is spent on cleaning the data. Real world data is always messy. Here are a few common discrepancies in most data-sets and some techniques of how to clean them:\n",
        "\n",
        "Missing values Missing values are values that are blank in the data-set. This can be due to various reasons like value being unknown, unrecorded, confidential etc. Since the reason for a value being missing is not clear, it is hard to guess the value.\n",
        "\n",
        "You could try different techniques to impute missing values starting with simple methods like column mean, median etc. and complex methods like using machine leaning models to estimate missing values.\n",
        "\n",
        "Duplicate records The challenge with duplicate records is identifying a record being duplicate. Duplicate records often occur while merging data from multiple sources. It could also occur due to human error. To identify duplicates, you could approximate a numeric values to certain decimal places and for text values, fuzzy matching could be a good start. Identification of duplicates could help the data engineering team to improve collection of data to prevent such errors.\n",
        "\n",
        "Incorrect values Incorrect values are mostly due to human error. For Eg. If there is a field called age and the value is 500, it is clearly wrong. Having domain knowledge of the data will help identify such values. A good technique to identify incorrect values for numerical columns could be to manually look at values beyond 3 standard deviations from the mean to check for correctness.\n",
        "\n",
        "* Integerizing\n",
        "Label Encoding vs One Hot"
      ]
    },
    {
      "metadata": {
        "_uuid": "a13559aa5f98e70d9cab54c714ac299a63b737f4",
        "id": "gv4ZroxB-A9a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-1-2Select numberical features and categorical features"
      ]
    },
    {
      "metadata": {
        "_uuid": "55651cdda3b812451c08875762b9708155f53ce2",
        "id": "MK6btxp5-A9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numberic_features=train.select_dtypes(include=[np.number])\n",
        "categorical_features=train.select_dtypes(include=[np.object])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "85d23b77d7fb1bb3e000535dc2475aca5862f242",
        "id": "H0HS6oFp-A9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-1-3 Target Value Analysis\n",
        "as you know **SalePrice** is our target value that we should predict it then now we take a look at it"
      ]
    },
    {
      "metadata": {
        "_uuid": "2d891ad744dec398f3920ce0fd9458c00671ed7b",
        "id": "rYzWHBZb-A9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train['SalePrice'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "09f6d29aa3a63f1d0d3033e7ef6b544fd8035889",
        "id": "GXpgNLmn-A95",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Flexibly plot a univariate distribution of observations.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "c7dceaddee057bb78a930c519221d3ad43eed360",
        "id": "xQbaMTOE-A98",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.distplot(train['SalePrice']);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a4fe40f3d3f57bf1369c7b9847c5c05ec704c0b6",
        "id": "HgmutSsY-A-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#skewness and kurtosis\n",
        "print(\"Skewness: %f\" % train['SalePrice'].skew())\n",
        "print(\"Kurtosis: %f\" % train['SalePrice'].kurt())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91dda1f631cf4ed362162501aaaac6d19cfd6cc7",
        "id": "XSmkUKIA-BB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-3 Data Preprocessing\n",
        "**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n",
        " \n",
        "Data Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\n",
        "there are plenty of steps for data preprocessing and we just listed some of them :\n",
        "* removing Target column (id)\n",
        "* Sampling (without replacement)\n",
        "* Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n",
        "* Introducing missing values and treating them (replacing by average values)\n",
        "* Noise filtering\n",
        "* Data discretization\n",
        "* Normalization and standardization\n",
        "* PCA analysis\n",
        "* Feature selection (filter, embedded, wrapper)"
      ]
    },
    {
      "metadata": {
        "_uuid": "1f8ae3a438f0fd7104974daa57b42b29f079ec45",
        "id": "gaX5qpEz-BB9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-3-1 removing ID"
      ]
    },
    {
      "metadata": {
        "_uuid": "3487e242b0c6ef83c93730c1b8eab2e231fff269",
        "id": "Wmwu7Rif-BB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save Id and drop it\n",
        "train_ID=train['Id']\n",
        "test_ID=test['Id']\n",
        "train.drop('Id',axis=1,inplace=True)\n",
        "test.drop('Id',axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "247350c88346f931d7987b4aacc3b55cae11d092",
        "id": "QyQ1vj7d-BDI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6-3-4 Feature selection\n",
        "let's first concatenate the train and test data in the same dataframe"
      ]
    },
    {
      "metadata": {
        "_uuid": "5584b10a50a819f6e5eb684efa01e69536d6e910",
        "id": "LtsdVVJj-BDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ntrain = train.shape[0]\n",
        "ntest = test.shape[0]\n",
        "y_train = train.SalePrice.values\n",
        "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
        "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
        "print(\"all_data size is : {}\".format(all_data.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58270b83638976da66c1aef6ba97017e01ef6dde",
        "id": "TS6xtGNp-BDQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-4 Data Cleaning\n",
        "When dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n",
        "\n",
        "The primary goal of data cleaning is to detect and remove errors and anomalies to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining.[8]"
      ]
    },
    {
      "metadata": {
        "_uuid": "4f37f9033ce20f8f60455419038b643e227f4eb7",
        "id": "lpaSn2zO-BDS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
        "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
        "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
        "missing_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcf718ed92a7d790f04ebcb5fe493109ad9a0d2d",
        "id": "Go9qhj_v-BDY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize=(15, 12))\n",
        "plt.xticks(rotation='90')\n",
        "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
        "plt.xlabel('Features', fontsize=15)\n",
        "plt.ylabel('Percent of missing values', fontsize=15)\n",
        "plt.title('Percent missing data by feature', fontsize=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acebef6cacdb2d377f8c66e15781021b4dd623bd",
        "id": "gRVfGLui-BDj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-4-1 Imputing missing values"
      ]
    },
    {
      "metadata": {
        "_uuid": "f4d374f64c50c29c534e286798033fd4586c929a",
        "id": "bszqbIJv-BDo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We impute them by proceeding sequentially through features with missing values\n",
        "\n",
        "PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. "
      ]
    },
    {
      "metadata": {
        "_uuid": "cb7ae811b45fd5288639b3933a7cb76117d9e918",
        "id": "rzQUeGr2-BDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a698f185d0bc10d50adad032d5aa4df6e971fc42",
        "id": "N_2Uaz0R-BDv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MiscFeature : data description says NA means \"no misc feature\""
      ]
    },
    {
      "metadata": {
        "_uuid": "63c20b645a9b8885ed0421ab0fbe4f7e762c1e80",
        "id": "IVVoHGC6-BDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6d8cdccabe631475c6911ff2ddacb0c8c0bb099d",
        "id": "wXe5LszA-BD6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Alley : data description says NA means \"no alley access\""
      ]
    },
    {
      "metadata": {
        "_uuid": "d513b815317dfa095ca32de6158fe34e3d8b630a",
        "id": "piKpwEuc-BD7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8df3b4eafb903e8961425d37f5a956f748454049",
        "id": "Wm0tNgwn-BEA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fence : data description says NA means \"no fence\""
      ]
    },
    {
      "metadata": {
        "_uuid": "ca718d780d6523767d8721ffe2a5776f3318c21e",
        "id": "RHAdg0q5-BEB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c00dc7a0fee9e12df4b679ce15acc0b68bdb9269",
        "id": "Lhbwf1XK-BEJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FireplaceQu : data description says NA means \"no fireplace\""
      ]
    },
    {
      "metadata": {
        "_uuid": "d264ec2eda6739c5468edec9524fbc88545ab023",
        "id": "9qIwnp_z-BEL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood."
      ]
    },
    {
      "metadata": {
        "_uuid": "043fd8463ce9e9ce9a204c36793cacb0bc9e7da7",
        "id": "rwhC-Oaq-BEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
        "all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
        "    lambda x: x.fillna(x.median()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a3de6297ee4b1c8aa191f371b7c4439c44dc6d17",
        "id": "MF5GBP6l-BER",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None"
      ]
    },
    {
      "metadata": {
        "_uuid": "9e891f0e9f74e1c73df4eb4b6998ca0145ef2cf4",
        "id": "p411QF2c-BEU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
        "    all_data[col] = all_data[col].fillna('None')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e187c90a90d3ef19ad70502804cbf5793811dea6",
        "id": "eaatlfNv-BEZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)"
      ]
    },
    {
      "metadata": {
        "_uuid": "2299992e24897468a34357457ced6e0a48ed4af9",
        "id": "G_z_s73H-BEa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
        "    all_data[col] = all_data[col].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a318163bdbb74721c4312ca50f07dc707be96421",
        "id": "pw1XWmtK-BEf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement"
      ]
    },
    {
      "metadata": {
        "_uuid": "626a388694a294d0e4a7196c30b78bcdcd76049f",
        "id": "X3PVDXC--BEm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
        "    all_data[col] = all_data[col].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f162adc10c70d93058400053d931117ada886366",
        "id": "s2UrLiAi-BEt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement."
      ]
    },
    {
      "metadata": {
        "_uuid": "3837452a0ff8a13fa9a586460de324990cde7681",
        "id": "-QBiNrRr-BEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
        "    all_data[col] = all_data[col].fillna('None')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aeb4b6ba3f5dc22d2611fdef40bf013689adf6cc",
        "id": "L99pohj--BE8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. "
      ]
    },
    {
      "metadata": {
        "_uuid": "5943301908ccdbc320559eebe0fa039768f483b0",
        "id": "5UxyZ-89-BE_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
        "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6995b801412966716240fedc95de48ff0fc30324",
        "id": "s-o4OuxW-BFJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'"
      ]
    },
    {
      "metadata": {
        "_uuid": "a3e9c2f67447bdc7bcff9cede08347e57ccd6fc8",
        "id": "tWs8FUzy-BFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bbed86b2cc9be58ccc55717819609fa942959c8a",
        "id": "eRnR6xu3-BFS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Functional : data description says NA means typical"
      ]
    },
    {
      "metadata": {
        "_uuid": "27c02c0abbb923d886107d3c905b7445c6be002a",
        "id": "bFdwiRNl-BFT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04a55104d376a28fea4b2c266fa85a93e98ffa09",
        "id": "R5IMQEzT-BFY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value."
      ]
    },
    {
      "metadata": {
        "_uuid": "c3c3270ff082eece4143bc770fc8e47f4d170868",
        "id": "hdEQnzAt-BFb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a606a784f53964b90f8d24378c29f5260d81d203",
        "id": "mkJpTy7--BFi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual."
      ]
    },
    {
      "metadata": {
        "_uuid": "b43419729c3a74bcfde64130805fbbd339d80ca0",
        "id": "nlRguvuD-BFl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c712c673465bed46fca7a5b66d35413fb1ba90d9",
        "id": "ZOAOGT1E-BFw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string"
      ]
    },
    {
      "metadata": {
        "_uuid": "7546dfbcafcc04f672654be1408eeb554882aef2",
        "id": "Y3S_-OBo-BFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
        "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c05bad0b6eedbf0ff8942f7662f2749e704e3c5a",
        "id": "FZpbTMYe-BF3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "SaleType : Fill in again with most frequent which is \"WD\""
      ]
    },
    {
      "metadata": {
        "_uuid": "60cb2511b84686d61d82a89b3915357eef08cafb",
        "id": "0sWJlaf--BF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d4ab263a1327656d5e9cb8d93f05e4438b1447b6",
        "id": "ukpd97nb-BGB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MSSubClass : Na most likely means No building class. We can replace missing values with None"
      ]
    },
    {
      "metadata": {
        "_uuid": "91b32f390489a4bf3002d45a170437cadc0799b6",
        "id": "r0UIl9hB-BGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fe8d0d7661c16446e9b3f8064ce9a7c9347833c2",
        "id": "eMdolX4C-BGL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FireplaceQu : data description says NA means \"no fireplace\""
      ]
    },
    {
      "metadata": {
        "_uuid": "a641088283207c78240be69c4df50b3199bbc4ac",
        "id": "idAtV1bY-BGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d51b94089f5c8fd6275a3baf26b0af3b71304053",
        "id": "vip3ewSJ-BGQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Is there any remaining missing value ?"
      ]
    },
    {
      "metadata": {
        "_uuid": "58706fd1525df4334577d7e5c8e502dd745af232",
        "id": "8tOX-ZTB-BGR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Check remaining missing values if any \n",
        "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
        "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
        "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
        "missing_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d41b34f251f139dc614e88b13b6732ff89ab2a7e",
        "id": "fyjS8gln-BGa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-4-2 More features engeneering\n",
        "\n",
        "Transforming some numerical variables that are really categorical"
      ]
    },
    {
      "metadata": {
        "_uuid": "62cb0d2fdf9f7c43145549f95b50fd3a72645db9",
        "id": "72QMOMMZ-BGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MSSubClass=The building class\n",
        "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
        "\n",
        "\n",
        "#Changing OverallCond into a categorical variable\n",
        "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
        "\n",
        "\n",
        "#Year and month sold are transformed into categorical features.\n",
        "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
        "all_data['MoSold'] = all_data['MoSold'].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8596a40b16f3b2ff70f89e9bf6c96d37d597c0bb",
        "id": "y6Uqac9g-BGk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Label Encoding some categorical variables that may contain information in their ordering set"
      ]
    },
    {
      "metadata": {
        "_uuid": "76dfc757e71406abfe2a46c6f9b5d851833173c8",
        "id": "qZ20XiRo-BGm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
        "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
        "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
        "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
        "        'YrSold', 'MoSold')\n",
        "# process columns, apply LabelEncoder to categorical features\n",
        "for c in cols:\n",
        "    lbl = LabelEncoder() \n",
        "    lbl.fit(list(all_data[c].values)) \n",
        "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
        "\n",
        "# shape        \n",
        "print('Shape all_data: {}'.format(all_data.shape))\n",
        "print(all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1323b314e494ba961bd78e1d37b66fd1b04b4848",
        "id": "iWd_17cn-BGs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adding one more important feature"
      ]
    },
    {
      "metadata": {
        "_uuid": "f06c3bf68559168f99b3e6d986555957871621c5",
        "id": "3oLBFrAx-BGu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house"
      ]
    },
    {
      "metadata": {
        "_uuid": "fd970993e0049ba8f5b5e245e59667a264a1c2ab",
        "colab_type": "text",
        "id": "phaVbqYI8hX3"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6-5 Skewed features"
      ]
    },
    {
      "metadata": {
        "_uuid": "e6084e1a715325e47d110bb30aed9b662b205444",
        "colab_type": "code",
        "id": "5Z2_3Igg8hX5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
        "\n",
        "# Check the skew of all numerical features\n",
        "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
        "print(\"\\nSkew in numerical features: \\n\")\n",
        "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
        "skewness.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dfa3549f9cca97eb1d11cc398b24f4c2f3bcc6e1",
        "colab_type": "text",
        "id": "lqGUAe8N8hX8"
      },
      "cell_type": "markdown",
      "source": [
        "Box Cox Transformation of (highly) skewed features\n",
        "\n",
        "We use the scipy function boxcox1p which computes the Box-Cox transformation of \n",
        "1+x\n",
        "1+x\n",
        ".\n",
        "Note that setting \n",
        "λ=0\n",
        "λ=0\n",
        "is equivalent to log1p used above for the target variable.\n",
        "See this page for more details on Box Cox Transformation as well as the scipy function's page"
      ]
    },
    {
      "metadata": {
        "_uuid": "cf4f2d574d61654a43bd5d51e905a43bee1a8a63",
        "colab_type": "code",
        "id": "aXyw0bCT8hX-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "skewness = skewness[abs(skewness) > 0.75]\n",
        "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
        "\n",
        "from scipy.special import boxcox1p\n",
        "skewed_features = skewness.index\n",
        "lam = 0.15\n",
        "for feat in skewed_features:\n",
        "    #all_data[feat] += 1\n",
        "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
        "    \n",
        "#all_data[skewed_features] = np.log1p(all_data[skewed_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8a5c392b2a526da1525c8a932371b09e7d17e72d",
        "colab_type": "text",
        "id": "xwL319NS8hYB"
      },
      "cell_type": "markdown",
      "source": [
        "Getting dummy categorical features"
      ]
    },
    {
      "metadata": {
        "_uuid": "289cf5d2051cdc8ecaf6c65526ddb57ca400a28a",
        "colab_type": "code",
        "id": "SZIQr8PD8hYD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data = pd.get_dummies(all_data)\n",
        "print(all_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bf69a592a66f806c9609d5d541701f8f94248531",
        "colab_type": "text",
        "id": "O6kgkDvG8hYH"
      },
      "cell_type": "markdown",
      "source": [
        "Getting the new train and test sets."
      ]
    },
    {
      "metadata": {
        "_uuid": "cc337f5030c5e10f798ea070e9db87ad205a219a",
        "colab_type": "code",
        "id": "mbJf7Ud48hYI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = all_data[:ntrain]\n",
        "test = all_data[ntrain:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GExdX60s0E_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression : One Variable"
      ]
    },
    {
      "metadata": {
        "id": "pOGDjgHhEA-G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Picking the target variable\n"
      ]
    },
    {
      "metadata": {
        "id": "iKnnRP9zOCs_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_set = open('dataset/train.csv')\n",
        "csv_training_set = csv.DictReader(training_set)\n",
        "\n",
        "total_sf = []\n",
        "sale_price = []\n",
        "\n",
        "for row in csv_training_set:\n",
        "  temp_total_sf = 0\n",
        "  temp_total_sf += float(row['TotalBsmtSF'])\n",
        "  temp_total_sf += float(row['1stFlrSF'])\n",
        "  temp_total_sf += float(row['2ndFlrSF'])\n",
        "  total_sf.append(temp_total_sf)\n",
        "  sale_price.append(row['SalePrice'])\n",
        "\n",
        "total_sf = np.array(total_sf, dtype='float64')\n",
        "sale_price = np.array(sale_price, dtype='float64')\n",
        "print('total square footage array:', total_sf)\n",
        "print('sale price array:', sale_price)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c92b300076a232321c915857d8a7c5685a97865",
        "id": "zEDyrD3i-BCK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x = total_sf, y = sale_price)\n",
        "plt.ylabel('SalePrice', fontsize=13)\n",
        "plt.xlabel('Total SF', fontsize=13)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1756ad43f10e017c7d3af76f640b8c948a65fe02",
        "id": "aHfk6PXE-BCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6-3-2 Noise filtering"
      ]
    },
    {
      "metadata": {
        "_uuid": "9269ae851b744856bce56840637030a16a5877e1",
        "colab_type": "code",
        "id": "k1UId9duVWLV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('dataset/train.csv')\n",
        "test= pd.read_csv('dataset/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e798f483a3a92e37830225d644a9656e34dcd5d6",
        "colab_type": "code",
        "id": "oJ214YQaVnMP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Adding total sqfootage feature \n",
        "train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "def67c793d2e365f07e90b190ae8211c0afd886c",
        "id": "lnMKp2b7-BCa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Deleting outliers\n",
        "train = train.drop(train[(train['TotalSF']>7000) & (train['SalePrice']<300000)].index)\n",
        "\n",
        "#Check the graphic again\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(train['TotalSF'], train['SalePrice'])\n",
        "plt.ylabel('SalePrice', fontsize=13)\n",
        "plt.xlabel('TotalSF', fontsize=13)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aijy_NYPNU2d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Cost Function"
      ]
    },
    {
      "metadata": {
        "id": "YitVDnZaNQgy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def computeCost(X,theta,y):\n",
        "    predicted = np.dot(theta, X)\n",
        "    cost = np.sum(np.square(predicted - y))\n",
        "    cost /= (2 * len(y))\n",
        "    return cost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goM87kLaNbe5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5JCYJC4VUKSM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "bNuJ3dImNo2e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, theta, alpha, num_iters):\n",
        "    # Performs gradient descent to learn theta\n",
        "    # updates theta by taking 'num_iters' gradient steps with learning rate 'alpha'\n",
        "\n",
        "    # initialize some useful values\n",
        "    m = len(y)  # number of training examples\n",
        "    cost_history = np.zeros(num_iters)\n",
        "\n",
        "    for iter in range(num_iters):\n",
        "        temp = np.sum((np.dot(theta, X) - y)*X, axis=1)\n",
        "\n",
        "        theta = theta - (alpha / m) * temp\n",
        "\n",
        "        cost_history[iter] = computeCost(X, theta, y)\n",
        "        \n",
        "    print('cost_history:', '\\n', cost_history)\n",
        "    step = [cost_history[idx+1]-cost for idx, cost in enumerate(cost_history) if idx<num_iters-1]\n",
        "    step = np.array(step)\n",
        "    print(step)\n",
        "    print('final theta:', theta)\n",
        "    \n",
        "    x = np.linspace(0, num_iters-2, num_iters-1)\n",
        "    y = np.abs(step)\n",
        "    print(x)\n",
        "    print('steps:', y)\n",
        "    plt.plot(x, y)\n",
        "    plt.show()\n",
        "    \n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LaAJJGUqUEma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ]
    },
    {
      "metadata": {
        "id": "sPVbM4qQUDF1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Gradient Descent\n",
        "#Some gradient descent settings\n",
        "iterations = 10\n",
        "alpha = 0.0000002\n",
        "\n",
        "theta = np.zeros(2)\n",
        "print('theta array:', theta)\n",
        "\n",
        "\n",
        "# initial cost\n",
        "# first row of X = ones\n",
        "num_training_ex = len(sale_price)\n",
        "print(num_training_ex)\n",
        "X = np.vstack((np.ones(num_training_ex),total_sf))\n",
        "print('first floor square footage array(ones inserted):''\\n', X)\n",
        "\n",
        "initial_cost = computeCost(X, theta, sale_price)\n",
        "print('initial cost:', initial_cost)\n",
        "\n",
        "\n",
        "# run gradient descent\n",
        "theta = gradient_descent(X, sale_price, theta, alpha, iterations)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_IJmBAMXw36",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting Performance"
      ]
    },
    {
      "metadata": {
        "id": "y6sz4z_-xFDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "\n",
        "train = train[train['LotArea'] < 30000]\n",
        "test = test[test['LotArea'] < 30000]\n",
        "\n",
        "X_train = train['LotArea']\n",
        "X_train = X_train.values.reshape(-1, 1)\n",
        "\n",
        "Y_train = train['SalePrice']\n",
        "Y_train = Y_train.values.reshape(-1, 1)\n",
        "\n",
        "X_test = test['LotArea']\n",
        "X_test = X_test.values.reshape(-1, 1)\n",
        "\n",
        "model.fit(X_train, Y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "plt.scatter(X_train, Y_train, color='red')\n",
        "plt.plot(X_test, pred, color='blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "183141b0c586eaf07701807196a48f792e10e374",
        "id": "uDfQPHfZ-BCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalizing the sale price\n",
        "SalePrice is the variable we need to predict. So let's do some analysis on this variable first."
      ]
    },
    {
      "metadata": {
        "_uuid": "8e9795273f50c45e5e582bf8af30facfb9087b8f",
        "id": "ZYTRQuX9-BCw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.distplot(train['SalePrice'] , fit=norm);\n",
        "\n",
        "# Get the fitted parameters used by the function\n",
        "(mu, sigma) = norm.fit(train['SalePrice'])\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "\n",
        "#Now plot the distribution\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
        "            loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "#Get also the QQ-plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train['SalePrice'], plot=plt)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1440b564cfa5fdd2059d684af5b1b1711658d929",
        "id": "1liNVsRK-BC7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "ce6f0b8460a6ed6e0170885e25cb8a9c15199746",
        "id": "v3QPJRno-BC8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Log-transformation of the target variable"
      ]
    },
    {
      "metadata": {
        "_uuid": "e13d451c514c35bf24c6bbd634bf6162cd0bfbed",
        "id": "pVzBDySq-BC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "\n",
        "#Check the new distribution \n",
        "sns.distplot(train['SalePrice'] , fit=norm);\n",
        "\n",
        "# Get the fitted parameters used by the function\n",
        "(mu, sigma) = norm.fit(train['SalePrice'])\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "\n",
        "#Now plot the distribution\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
        "            loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "#Get also the QQ-plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train['SalePrice'], plot=plt)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4WRcpLTETSO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression : Multiple Variables"
      ]
    },
    {
      "metadata": {
        "id": "NzpPg4BwYCcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_set = open('dataset/train.csv')\n",
        "csv_training_set = csv.DictReader(training_set)\n",
        "\n",
        "feature_list = [\n",
        "  'OverallQual',\n",
        "  'YearBuilt',\n",
        "  'TotalBsmtSF',\n",
        "  '1stFlrSF',\n",
        "  '2ndFlrSF',\n",
        "  'FullBath'\n",
        "]\n",
        "\n",
        "nb_features = len(feature_list)\n",
        "feature_matrix = np.empty([nb_features,1])\n",
        "sale_price = []\n",
        "num_training_ex = 0\n",
        "\n",
        "for idx, row in enumerate(csv_training_set):\n",
        "    feature_column = []\n",
        "    for feature in feature_list:\n",
        "        #if row[feature] == 'NA':\n",
        "        #    row[feature] = 0\n",
        "        feature_column.append(row[feature])\n",
        "    feature_vector = np.array([feature_column], dtype='float64').T\n",
        "    if idx is 0:\n",
        "        feature_matrix = feature_vector\n",
        "    else:\n",
        "        feature_matrix = np.concatenate([feature_matrix, feature_vector], axis=1)\n",
        "    sale_price.append(row['SalePrice'])\n",
        "\n",
        "sale_price = np.array(sale_price, dtype='float64')\n",
        "print('Features Matrix:', '\\n', feature_matrix)\n",
        "print('Sale Price Array:', '\\n', sale_price)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82SH_tUTZ9OU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Feature Scaling!')\n",
        "\n",
        "for i, column in enumerate(feature_matrix):\n",
        "    max = np.max(column)\n",
        "    min = np.min(column)\n",
        "    spread = max - min\n",
        "    mean = np.mean(column)\n",
        "    # print('Column before Scaling:', column)\n",
        "\n",
        "    for j, content in enumerate(column):\n",
        "        column[j] = (content-mean)/spread\n",
        "    # print('Column after Scaling:', column)\n",
        "    feature_matrix[i] = column\n",
        "\n",
        "#print('Feature Matrix After Scaling:', '\\n', feature_matrix)\n",
        "\"\"\"\n",
        "max = np.max(sale_price)\n",
        "min = np.min(sale_price)\n",
        "spread = max - min\n",
        "mean = np.mean(column)\n",
        "\n",
        "for idx, entry in enumerate(sale_price):\n",
        "  sale_price[idx] = (entry - mean)/spread\n",
        "\"\"\"\n",
        "print(sale_price)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hSkyituaJm-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Gradient Descent\n",
        "#Some gradient descent settings\n",
        "iterations = 5000\n",
        "alpha = 1.5\n",
        "\n",
        "theta = np.zeros(nb_features+1)\n",
        "print('theta array:', theta)\n",
        "\n",
        "\n",
        "# initial cost\n",
        "# first row of X = ones\n",
        "num_training_ex = len(sale_price) \n",
        "X = np.vstack((np.ones(num_training_ex),feature_matrix))\n",
        "print('first floor square footage array(ones inserted):''\\n', X)\n",
        "\n",
        "current_cost = computeCost(X, theta, sale_price)\n",
        "print('initial cost:', current_cost)\n",
        "\n",
        "\n",
        "# run gradient descent\n",
        "theta = gradient_descent(X, sale_price, theta, alpha, iterations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xdvnFZiOyxQm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = ['LotArea', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd']\n",
        "\n",
        "X_train = train[features]\n",
        "X_test = test[features]\n",
        "\n",
        "Y_train = train['SalePrice']\n",
        "Y_train = Y_train.values.reshape(-1, 1)\n",
        "\n",
        "model.fit(X_train, Y_train)\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qSyh614pEcjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Polynomial Regression\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J5kTGxBj0KvE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Overfitting"
      ]
    },
    {
      "metadata": {
        "id": "3yNT-K79Ejjy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "KydTXCWadfHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# More Regression Algorithms (Extra Material)"
      ]
    },
    {
      "metadata": {
        "_uuid": "e3e85e82fe0cb48e92f5c28b416bc268f2308404",
        "id": "w72ECgkY-BHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
        "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2a62eeab41a10c4dce5a17d519325e19b928c1c",
        "id": "b8EENeyd-BHg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7-1 Defining a cross validation strategy"
      ]
    },
    {
      "metadata": {
        "_uuid": "361202ee3e187da7bdfedfe0a22ccee402fc5529",
        "id": "0CGjoqMf-BHk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use the cross_val_score function of **Sklearn**. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation"
      ]
    },
    {
      "metadata": {
        "_uuid": "8c0a3918a57cf88a08f7784d71271bdda86a8b1a",
        "id": "R9hppoZx-BHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Validation function\n",
        "n_folds = 5\n",
        "\n",
        "def rmsle_cv(model):\n",
        "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
        "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
        "    return(rmse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8750d7d0b0d3c19b43fc27beec890055022635df",
        "id": "unPcz0A1-BHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7-2 Models"
      ]
    },
    {
      "metadata": {
        "_uuid": "23e42d6030d979df79b81ea58da45effc31f0d4d",
        "id": "YM68Gia7-BHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### LASSO Regression \n",
        "In statistics and machine learning, lasso (least absolute shrinkage and selection operator)  is a **regression analysis** method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.  Lasso was originally formulated for least squares models and this simple case reveals a substantial amount about the behavior of the estimator, including its relationship to ridge regression and best subset selection and the connections between lasso coefficient estimates and so-called soft thresholding. It also reveals that (like standard linear regression) the coefficient estimates need not be unique if covariates are collinear.\n",
        "\n",
        "This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline"
      ]
    },
    {
      "metadata": {
        "_uuid": "a0edc93fc6298e4696eaab1430467b8262037f2a",
        "id": "9O2uyNfP-BHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e6084e1a715325e47d110bb30aed9b662b205444",
        "id": "ChA7L_mA-BG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
        "\n",
        "# Check the skew of all numerical features\n",
        "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
        "print(\"\\nSkew in numerical features: \\n\")\n",
        "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
        "skewness.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8273cf674a3d93d8d5edc8849ea0ca81d80c453c",
        "id": "PD8lqE-M-BHw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Elastic Net Regression \n",
        "the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.\n",
        "again made robust to outliers"
      ]
    },
    {
      "metadata": {
        "_uuid": "36065ec44f99bffc2e61904d444918a3f8d15104",
        "id": "TE9AkABE-BHz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e92dca79ca3291620ba7d07de3c9879030aee90",
        "id": "BfT8PT8J-BH4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Kernel Ridge Regression \n",
        "Kernel ridge regression (KRR)  combines Ridge Regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space."
      ]
    },
    {
      "metadata": {
        "_uuid": "d9242099a3f75392efe6dfe23a06f8b472bbf6b0",
        "id": "VMyreKgg-BH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b2f64974b200069e751844090c32044a2ab08aa",
        "id": "PK0WNrQy-BH_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Gradient Boosting Regression\n",
        "With huber loss that makes it robust to outliers"
      ]
    },
    {
      "metadata": {
        "_uuid": "29f20b79c32d0083171671453399bf28f0677ce2",
        "id": "t5TzK_Rg-BID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
        "                                   max_depth=4, max_features='sqrt',\n",
        "                                   min_samples_leaf=15, min_samples_split=10, \n",
        "                                   loss='huber', random_state =5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "55a55a6cfca539fd9a8b110ab6143a09e4962972",
        "id": "Q1i7rfd6-BIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "metadata": {
        "_uuid": "64f958e8e7e3c4880eb23a7f654f405a3dbff163",
        "id": "dh9dNSSl-BIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
        "                             learning_rate=0.05, max_depth=3, \n",
        "                             min_child_weight=1.7817, n_estimators=2200,\n",
        "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
        "                             subsample=0.5213, silent=1,\n",
        "                             random_state =7, nthread = -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1fef6eaaf1cf5780ec134cf3e1017750e500dfc7",
        "id": "jjFkN-UX-BIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### LightGBM"
      ]
    },
    {
      "metadata": {
        "_uuid": "b08f8cb3235e85ccd962dd26c030ca38c6c1ddb5",
        "id": "5ub_TkSK-BIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
        "                              learning_rate=0.05, n_estimators=720,\n",
        "                              max_bin = 55, bagging_fraction = 0.8,\n",
        "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4811c189f288825b7a3d4a73589bea75966ad6f8",
        "id": "g4NYZrk7-BIS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Base models scores\n",
        "\n",
        "Let's see how these base models perform on the data by evaluating the cross-validation rmsle error"
      ]
    },
    {
      "metadata": {
        "_uuid": "432cfefb8fd0481baa754ee142900da68e1242a1",
        "id": "8k6DlHkg-BIU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = rmsle_cv(lasso)\n",
        "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1a1b1442588e8027fa3e322336d429a6bd0a0698",
        "id": "TTRofPuw-BIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = rmsle_cv(ENet)\n",
        "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2733161f02721cfedf6fb87b75c4ec02373c11b6",
        "id": "sMv7yuGn-BIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = rmsle_cv(KRR)\n",
        "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f47e020b29a530a41262d1e303c30db37b73adc7",
        "id": "CiogKACq-BIl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = rmsle_cv(GBoost)\n",
        "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2a6b8ae0750efcb82851f5bca5fe1052e105a78",
        "id": "Ghow0M1e-BIp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = rmsle_cv(model_xgb)\n",
        "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "abc7b6c547d39419c7b61924cfea81b55f353a4c",
        "id": "cCeZ2YYI-BIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = rmsle_cv(model_lgb)\n",
        "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "feb056008e799b45ef4c34b09a288606c9587857",
        "id": "slEQ8qlS-BI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Stacking models"
      ]
    },
    {
      "metadata": {
        "_uuid": "bce02781b5a07d0fdd1e5f53908d2d7c7410b1a3",
        "id": "3XRuP7uX-BI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Simplest Stacking approach : Averaging base models\n",
        "We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)"
      ]
    },
    {
      "metadata": {
        "_uuid": "47bf4fec6d2a926a04b3eecfe4c32a092b00037f",
        "id": "jMF2mcI3-BI5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Averaged base models class"
      ]
    },
    {
      "metadata": {
        "_uuid": "45f494d3bf43f0e7d54802555ea75e52c3d71a0b",
        "id": "qyGa89NJ-BI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "        \n",
        "    # we define clones of the original models to fit the data in\n",
        "    def fit(self, X, y):\n",
        "        self.models_ = [clone(x) for x in self.models]\n",
        "        \n",
        "        # Train cloned base models\n",
        "        for model in self.models_:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    #Now we do the predictions for cloned models and average them\n",
        "    def predict(self, X):\n",
        "        predictions = np.column_stack([\n",
        "            model.predict(X) for model in self.models_\n",
        "        ])\n",
        "        return np.mean(predictions, axis=1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a277422c2957a9780a723b78d3420dded90876c",
        "id": "wxPKEqg_-BJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Averaged base models score\n",
        "\n",
        "We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix."
      ]
    },
    {
      "metadata": {
        "_uuid": "219536044d4f86e0787854c61e47fe066d7c74ea",
        "id": "JqfHfqSY-BJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
        "\n",
        "score = rmsle_cv(averaged_models)\n",
        "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7c6a15fce609c765ea854ac4b6b159eaad681a1",
        "id": "WHlx1TMV-BJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "   # We again fit the data on clones of the original models\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for x in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
        "        \n",
        "        # Train cloned base models then create out-of-fold predictions\n",
        "        # that are needed to train the cloned meta-model\n",
        "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                self.base_models_[i].append(instance)\n",
        "                instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = instance.predict(X[holdout_index])\n",
        "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
        "                \n",
        "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
        "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
        "        return self\n",
        "   #Do the predictions of all base models on the test data and use the averaged predictions as \n",
        "    #meta-features for the final prediction which is done by the meta-model\n",
        "    def predict(self, X):\n",
        "        meta_features = np.column_stack([\n",
        "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
        "            for base_models in self.base_models_ ])\n",
        "        return self.meta_model_.predict(meta_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4906e53b1fcf685b054ea884cc2281b613a78918",
        "id": "4O7q4B8d-BJM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
        "                                                 meta_model = lasso)\n",
        "\n",
        "score = rmsle_cv(stacked_averaged_models)\n",
        "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a67243faf709d764428b6b7451aa61e917da0bab",
        "id": "-RxWTLvV-BJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rmsle(y, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "45edc8ed4de1313734facc50567a74ccf7f7075b",
        "id": "RdXh4W4H-BJT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#StackedRegressor\n",
        "#Final Training and Prediction\n",
        "stacked_averaged_models.fit(train.values, y_train)\n",
        "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
        "stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
        "print(rmsle(y_train, stacked_train_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca83502d37f38b6cce5928220538749982352f2e",
        "id": "fV2_3655-BJW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "model_xgb.fit(train, y_train)\n",
        "xgb_train_pred = model_xgb.predict(train)\n",
        "xgb_pred = np.expm1(model_xgb.predict(test))\n",
        "print(rmsle(y_train, xgb_train_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9611bf793492523c101226912037b78ad54b2bdc",
        "id": "FtJ_kTEa-BJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lightGBM\n",
        "model_lgb.fit(train, y_train)\n",
        "lgb_train_pred = model_lgb.predict(train)\n",
        "lgb_pred = np.expm1(model_lgb.predict(test.values))\n",
        "print(rmsle(y_train, lgb_train_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "435ff580ab2d73f4709ad871af48f254ddabbc7b",
        "id": "46Ic2UIj-BJv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''RMSE on the entire Train data when averaging'''\n",
        "\n",
        "print('RMSLE score on train data:')\n",
        "print(rmsle(y_train,stacked_train_pred*0.70 +\n",
        "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9e18ae69a5e4a12f5b4042f91e65b88da7d1b6ba",
        "id": "-IY6qjb0-BJz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ensemble prediction"
      ]
    },
    {
      "metadata": {
        "_uuid": "f82001f0b991dd92f701d9781e2fe5c5dbfbdaac",
        "id": "ZgVjdOgn-BJ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b1839af2f826485972714196d8ea8ee17400808e",
        "id": "qHcNFj_6-BJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sub = pd.DataFrame()\n",
        "sub['Id'] = test_ID\n",
        "sub['SalePrice'] = ensemble\n",
        "sub.to_csv('submission.csv',index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dfa3549f9cca97eb1d11cc398b24f4c2f3bcc6e1",
        "id": "SdBKzjMH-BG5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Box Cox Transformation of (highly) skewed features\n",
        "\n",
        "We use the scipy function boxcox1p which computes the Box-Cox transformation of \n",
        "1+x\n",
        "1+x\n",
        ".\n",
        "Note that setting \n",
        "λ=0\n",
        "λ=0\n",
        "is equivalent to log1p used above for the target variable.\n",
        "See this page for more details on Box Cox Transformation as well as the scipy function's page"
      ]
    },
    {
      "metadata": {
        "_uuid": "cf4f2d574d61654a43bd5d51e905a43bee1a8a63",
        "id": "m-yZmwpx-BG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "skewness = skewness[abs(skewness) > 0.75]\n",
        "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
        "\n",
        "from scipy.special import boxcox1p\n",
        "skewed_features = skewness.index\n",
        "lam = 0.15\n",
        "for feat in skewed_features:\n",
        "    #all_data[feat] += 1\n",
        "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
        "    \n",
        "#all_data[skewed_features] = np.log1p(all_data[skewed_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "97adc471c068fbd8d36ca19a4db0d98b0924c731",
        "id": "gPrGWNhN-BKJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-----------------\n",
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "id": "wl11WN3H-BKO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 9- References\n",
        "* [1] https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-for-house-prices\n",
        "* [2] [https://skymind.ai/wiki/machine-learning-workflow](https://skymind.ai/wiki/machine-learning-workflow)\n",
        "* [3] [Problem-define](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n",
        "* [4] [Sklearn](http://scikit-learn.org/)\n",
        "* [5] [machine-learning-in-python-step-by-step](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n",
        "* [6] [Data Cleaning](http://wp.sigmod.org/?p=2288)\n",
        "* [7] [kaggle kernel](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n",
        "\n"
      ]
    }
  ]
}